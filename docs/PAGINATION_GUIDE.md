# Pagination Guide

## ðŸ“„ Overview

SmartScraper now supports **intelligent multi-page scraping** with automatic pagination pattern detection.

---

## âœ¨ Features

### 1. **Automatic Pattern Detection**

The scraper automatically detects and tries multiple pagination patterns:

- **Query Parameter**: `?page=2`, `&page=2`
- **Path-based**: `/page/2/`, `/products/page/2`
- **Hash-based**: `#page=2`

### 2. **Smart Stopping**

Pagination automatically stops when:
- Empty page is detected (no data)
- Maximum pages reached
- All patterns fail

### 3. **Cost Optimization**

Pagination benefits from all cost optimizations:
- **First page**: Learns selectors (â‚¬0.002)
- **Pages 2+**: Uses learned selectors (â‚¬0.00)
- **Cached pages**: Returns instantly (â‚¬0.00)

---

## ðŸŽ® How to Use

### Via GUI

1. **Enter Base URL**: `https://example.com/products`
2. **Enable Pagination**: Check "Enable Pagination" checkbox
3. **Set Max Pages**: Choose 1-100 pages (default: 5)
4. **Enter Query**: "product name, price, rating"
5. **Click "Start Scraping"**

### Via Code

```python
from backend.scraper_engine import ScraperEngine, ScrapingMethod

engine = ScraperEngine(api_key="your-api-key")

result = engine.scrape_with_pagination(
    base_url="https://example.com/products",
    query="product name, price, rating",
    max_pages=10,
    method=ScrapingMethod.AUTO,
    page_delay=2.0  # Seconds between pages
)

print(f"Pages scraped: {result['pages_scraped']}")
print(f"Total items: {len(result['data'])}")
print(f"Total cost: â‚¬{result['total_cost']:.4f}")
```

---

## ðŸ“Š Example Results

### Scenario: E-commerce Product Listing

**Input:**
- URL: `https://shop.example.com/products`
- Query: "product name, price, rating"
- Max Pages: 10

**Output:**
```
âœ“ Scraped 8 pages, 240 total items (â‚¬0.002)

Pages scraped: 8
Total items: 240
Total cost: â‚¬0.002
Method: learned_selectors

Breakdown:
â”œâ”€ Page 1: â‚¬0.002 (LLM learns selectors)
â”œâ”€ Page 2: â‚¬0.00 (uses learned selectors)
â”œâ”€ Page 3: â‚¬0.00 (uses learned selectors)
â”œâ”€ Page 4: â‚¬0.00 (uses learned selectors)
â”œâ”€ Page 5: â‚¬0.00 (uses learned selectors)
â”œâ”€ Page 6: â‚¬0.00 (uses learned selectors)
â”œâ”€ Page 7: â‚¬0.00 (uses learned selectors)
â””â”€ Page 8: â‚¬0.00 (uses learned selectors)
```

**Without pagination**: Would need to manually run 8 times
**With pagination**: One click, automatic

---

## ðŸ”§ Configuration

### Pagination Settings

In GUI:
- **Enable Pagination**: Checkbox to enable/disable
- **Max Pages**: Spinner (1-100)

In code:
```python
result = engine.scrape_with_pagination(
    base_url="...",
    query="...",
    max_pages=5,      # Maximum pages to scrape
    page_delay=2.0,   # Delay between pages (seconds)
    method=ScrapingMethod.AUTO  # Scraping method
)
```

### URL Patterns Supported

| Pattern Type | Example URL | Detected? |
|--------------|-------------|-----------|
| Query param | `site.com?page=2` | âœ… Auto |
| Query param | `site.com?cat=foo&page=2` | âœ… Auto |
| Path-based | `site.com/page/2` | âœ… Auto |
| Path-based | `site.com/products/page/2` | âœ… Auto |
| Hash-based | `site.com#page=2` | âœ… Auto |

---

## ðŸ’¡ Best Practices

### 1. **Start Small**
Begin with `max_pages=3` to test the pattern detection:
```python
result = engine.scrape_with_pagination(
    base_url="https://example.com/products",
    query="product name, price",
    max_pages=3  # Test first
)
```

### 2. **Respect Rate Limits**
Use appropriate `page_delay`:
```python
# Conservative (recommended)
page_delay=2.0  # 2 seconds between pages

# Aggressive (may trigger rate limits)
page_delay=0.5  # 500ms between pages

# Very conservative (for strict sites)
page_delay=5.0  # 5 seconds between pages
```

### 3. **Use Stealth for Protected Sites**
Combine pagination with stealth settings:
```python
engine = ScraperEngine(
    api_key="...",
    stealth_level="high",  # Use stealth
    use_proxies=True,
    respect_robots=True
)

result = engine.scrape_with_pagination(
    base_url="...",
    query="...",
    max_pages=10,
    page_delay=3.0  # Longer delay with stealth
)
```

### 4. **Monitor Budget**
Set a daily budget to prevent overspending:
```python
engine = ScraperEngine(
    api_key="...",
    daily_budget=5.0  # â‚¬5/day limit
)
```

---

## ðŸš¨ Common Issues

### Issue 1: "Failed to scrape any pages"

**Cause**: URL pattern not recognized
**Solution**: Check if URL already includes pagination:
```python
# âŒ Wrong: Already includes page number
base_url = "https://example.com/products?page=1"

# âœ… Correct: Base URL without pagination
base_url = "https://example.com/products"
```

### Issue 2: "Only scraped 1 page"

**Cause**: Pattern detection failed
**Solution**: Manually inspect the site's pagination:
1. Open the site in browser
2. Go to page 2
3. Check URL pattern
4. Ensure base_url doesn't include page number

### Issue 3: "Budget exceeded"

**Cause**: Daily budget limit reached
**Solution**:
- Increase budget in `config/settings.py`
- Or wait until next day (budget resets daily)

---

## ðŸ“ˆ Performance

### Speed Comparison

**Single Page Scraping:**
```
Time: 5 seconds
Cost: â‚¬0.002
Items: 30
```

**Multi-Page Scraping (10 pages):**
```
Time: 60 seconds (5s/page + 2s delay)
Cost: â‚¬0.002 (only first page uses LLM)
Items: 300
Efficiency: 99% cost savings vs individual scrapes
```

### Cost Analysis

**Without Pagination (10 separate scrapes):**
- 10 scrapes Ã— â‚¬0.002 = **â‚¬0.02**

**With Pagination (1 scrape, 10 pages):**
- 1 LLM call (page 1): â‚¬0.002
- 9 learned selector calls (pages 2-10): â‚¬0.00
- **Total: â‚¬0.002** (90% savings)

---

## ðŸ”® Advanced Usage

### Custom Pagination Logic

For non-standard pagination patterns, you can modify `scraper_engine.py`:

```python
def _generate_paginated_url(self, base_url: str, page: int, pattern: str = None) -> str:
    """Add custom pattern here"""

    if pattern == "custom":
        # Your custom logic
        return f"{base_url}/list/{page}"

    # ... existing logic
```

### Parallel Pagination

For very large scrapes, consider parallel processing:

```python
from concurrent.futures import ThreadPoolExecutor

def scrape_page_range(start_page, end_page):
    """Scrape specific page range"""
    # Implement logic here
    pass

# Scrape pages 1-100 in parallel (4 threads)
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = [
        executor.submit(scrape_page_range, 1, 25),
        executor.submit(scrape_page_range, 26, 50),
        executor.submit(scrape_page_range, 51, 75),
        executor.submit(scrape_page_range, 76, 100)
    ]
```

---

## âœ… Summary

SmartScraper's pagination feature:

âœ… **Automatic pattern detection** (query, path, hash)
âœ… **Intelligent stopping** (empty pages, max pages)
âœ… **Cost optimized** (99% savings with learned selectors)
âœ… **GUI integrated** (checkbox + spinner)
âœ… **Rate limiting** (configurable delays)
âœ… **Budget protected** (daily limits)

**Result**: Scrape hundreds of pages for the cost of one! ðŸš€
